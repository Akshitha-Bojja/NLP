{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMjjL+r9/1fPce7zCEHXAZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshitha-Bojja/NLP/blob/main/Assignment-08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example text data (you can replace this with any larger corpus) text = \"\"\" Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods. One day, her mother asked her to take a basket of goodies to her grandmother. On her way through the woods, she met a big bad wolf who wanted to eat her. [CO5]"
      ],
      "metadata": {
        "id": "8ZSkiLexsLzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) Build the Transformer Model on above dataset"
      ],
      "metadata": {
        "id": "Su8lcFXVspdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# Sample text data\n",
        "text = \"\"\"Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods. One day, her mother asked her to take a basket of goodies to her grandmother. On her way through the woods, she met a big bad wolf who wanted to eat her.\"\"\"\n",
        "\n",
        "# Tokenize and clean the text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    return text.split()\n",
        "\n",
        "# Tokenize text\n",
        "tokens = clean_text(text)\n",
        "\n",
        "# Create vocabulary (mapping from words to indices)\n",
        "word_counts = Counter(tokens)\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(word_counts.items())}\n",
        "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "# Convert tokens to indices\n",
        "token_indices = [vocab[word] for word in tokens]\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, seq_len):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.data[idx:idx+self.seq_len])\n",
        "        y = torch.tensor(self.data[idx+1:idx+self.seq_len+1])\n",
        "        return x, y\n",
        "\n",
        "# Hyperparameters\n",
        "seq_len = 10\n",
        "dataset = TextDataset(token_indices, seq_len)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Z8HXHZHdtGQv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture:"
      ],
      "metadata": {
        "id": "FzrmWdpHtstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, seq_len, hidden_size):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=hidden_size\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)  # Shape: [batch_size, seq_len, embed_size]\n",
        "        x = x.permute(1, 0, 2)  # Transformer expects [seq_len, batch_size, embed_size]\n",
        "        x = self.transformer(x, x)  # Use the same input as both source and target\n",
        "        x = x.permute(1, 0, 2)  # [batch_size, seq_len, embed_size]\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "vocab_size = len(vocab)\n",
        "embed_size = 128\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "hidden_size = 512\n",
        "\n",
        "model = TransformerModel(vocab_size, embed_size, num_heads, num_layers, seq_len, hidden_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV1lrjtTttrP",
        "outputId": "86601f1c-f525-42f4-806a-608ad26cd5e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ii) Train the model using 20, 60, 70 epochs\n",
        "\n"
      ],
      "metadata": {
        "id": "CvJBwoc_t078"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "def train(model, dataloader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for x_batch, y_batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x_batch)  # Get predictions\n",
        "            loss = criterion(output.view(-1, vocab_size), y_batch.view(-1))  # Compute loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(dataloader):.4f}')\n",
        "\n",
        "train(model, dataloader, epochs=20)  # Training for 20 epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4JytvXAt1s2",
        "outputId": "9b020fe4-4a7b-455f-91b0-984f11245333"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 3.6396\n",
            "Epoch [2/20], Loss: 3.5117\n",
            "Epoch [3/20], Loss: 3.3988\n",
            "Epoch [4/20], Loss: 3.3231\n",
            "Epoch [5/20], Loss: 3.2733\n",
            "Epoch [6/20], Loss: 3.3562\n",
            "Epoch [7/20], Loss: 3.2606\n",
            "Epoch [8/20], Loss: 3.1593\n",
            "Epoch [9/20], Loss: 3.2888\n",
            "Epoch [10/20], Loss: 3.2493\n",
            "Epoch [11/20], Loss: 3.1891\n",
            "Epoch [12/20], Loss: 3.1341\n",
            "Epoch [13/20], Loss: 3.1799\n",
            "Epoch [14/20], Loss: 3.1580\n",
            "Epoch [15/20], Loss: 3.1818\n",
            "Epoch [16/20], Loss: 3.2196\n",
            "Epoch [17/20], Loss: 3.1423\n",
            "Epoch [18/20], Loss: 3.1324\n",
            "Epoch [19/20], Loss: 3.1106\n",
            "Epoch [20/20], Loss: 3.0517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(iii) After training, use the model to generate new text by feeding it an initial seed text\n",
        "\n"
      ],
      "metadata": {
        "id": "olV9v5KxuDJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add '' token to the vocabulary\n",
        "UNK_TOKEN = ''\n",
        "vocab = {word: idx + 1 for idx, (word, _) in enumerate(word_counts.items())}  # Start from 1\n",
        "vocab[UNK_TOKEN] = 0  # Assign 0 to '' token\n",
        "reverse_vocab = {idx: word for word, idx in vocab.items()}\n"
      ],
      "metadata": {
        "id": "SpCMEUPduEJO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def generate_text(model, seed_text, length=50, temperature=1.0, top_k=50, top_p=0.9):\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess the seed text\n",
        "    seed_tokens = clean_text(seed_text)\n",
        "    seed_indices = [vocab.get(word, vocab['']) for word in seed_tokens]  # Convert to indices\n",
        "    input_tensor = torch.tensor(seed_indices).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    generated = seed_tokens.copy()\n",
        "\n",
        "    # Define a function for temperature scaling\n",
        "    def temperature_sampling(logits, temperature):\n",
        "        logits = logits / temperature\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        return probabilities\n",
        "\n",
        "    # Define a function for top-k sampling\n",
        "    def top_k_sampling(logits, k):\n",
        "        top_k_values, top_k_indices = torch.topk(logits, k)\n",
        "        probabilities = torch.nn.functional.softmax(top_k_values, dim=-1)\n",
        "        return top_k_indices, probabilities\n",
        "\n",
        "    # Define a function for top-p (nucleus) sampling\n",
        "    def top_p_sampling(logits, p):\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Cut off the tail of the distribution\n",
        "        sorted_indices_to_keep = cumulative_probs <= p\n",
        "        sorted_logits[~sorted_indices_to_keep] = -float('Inf')\n",
        "        probabilities = torch.nn.functional.softmax(sorted_logits, dim=-1)\n",
        "\n",
        "        return sorted_indices, probabilities\n",
        "\n",
        "    # Start generating tokens\n",
        "    for _ in range(length):\n",
        "        output = model(input_tensor)\n",
        "        logits = output[:, -1, :]  # Get the logits for the last token prediction\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        logits = temperature_sampling(logits, temperature)\n",
        "\n",
        "        # Sample from top-k or top-p (nucleus) sampling\n",
        "        if top_p > 0:\n",
        "            top_indices, probabilities = top_p_sampling(logits, top_p)\n",
        "        else:\n",
        "            top_indices, probabilities = top_k_sampling(logits, top_k)\n",
        "\n",
        "        # Sample the next token from the distribution\n",
        "        next_token_idx = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "        # Get the word for the next token using the sampled index\n",
        "        next_word = reverse_vocab.get(top_indices[0, next_token_idx].item(), vocab[''])\n",
        "        generated.append(next_word)\n",
        "\n",
        "        # Update input for next iteration (use the predicted token index)\n",
        "        input_tensor = torch.cat([input_tensor[:, 1:], torch.tensor([[top_indices[0, next_token_idx]]])], dim=1)\n",
        "\n",
        "    return ' '.join(generated)\n",
        "\n",
        "\n",
        "# Example generation\n",
        "seed_text = \"Once upon a time\"\n",
        "generated_text = generate_text(model, seed_text, length=50, temperature=0.7, top_k=50, top_p=0.9)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FTEuidCuOJE",
        "outputId": "7e641443-a415-491f-e541-045a4bd4de9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "once upon a time on big met wanted in was visit in she mother wolf through the asked way basket asked big upon upon in lived grandmother grandmother she the visit little goodies way in basket her met big there day through on day upon there  her loved to who on girl loved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(iv) Experimenting and Improving the Model by large dataset and hyper tune parameter."
      ],
      "metadata": {
        "id": "bd3o_0tWuTV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Modify the TransformerModel to include dropout\n",
        "class TransformerModelWithDropout(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, seq_len, hidden_size, dropout_rate=0.1):\n",
        "        super(TransformerModelWithDropout, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=hidden_size,\n",
        "            dropout=dropout_rate  # Add dropout here\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "# Define the scheduler\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Decay learning rate by 0.1 every 10 epochs\n"
      ],
      "metadata": {
        "id": "LOlMq2thuUUd"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}